{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3oN73AEhkeq62Y9dFNDjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guipantiga/CRMAnalysis/blob/main/Collaborative_Filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customer Relationship Management (CRM) - Collaborative Filtering using Spark\n",
        "\n",
        "Collaborative filtering (CF) is a technique used by recommender systems. Collaborative filtering has two senses, a narrow one and a more general one.\n",
        "\n",
        "In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person.\n",
        "\n",
        "### About\n",
        "\n",
        "*   Data source: https://www.kaggle.com/datasets/jihyeseo/online-retail-data-set-from-uci-ml-repo\n",
        "\n"
      ],
      "metadata": {
        "id": "MS4_ZTseL0-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "yKWshzQwMOKM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "-Z1TQdc7MO4D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "from pyspark.ml.feature import QuantileDiscretizer, Bucketizer\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Colab\").getOrCreate()\n",
        "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
        "\n",
        "# Check Spark Session Information\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "EY-V9j5vMPFG",
        "outputId": "46ea6168-8870-4ae6-a2d7-94780a8521c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd1564be9e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ff495718e625:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rawDF = spark.read.csv('/content/data.csv', encoding=\"ISO-8859-1\", header=True)"
      ],
      "metadata": {
        "id": "Zvcqs7wTMYBz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin the previous treatment for Online Retail\n",
        "# Filtering\n",
        "# Cleaning and formating\n",
        "# Renaming Columns\n",
        "# Creating Columns\n",
        "# Aggregating\n",
        "\n",
        "df = (rawDF\n",
        "      .filter(\n",
        "          (F.col(\"CustomerID\").isNotNull())\n",
        "          & (F.col('Quantity') > 0)\n",
        "          & (F.col('UnitPrice') > 0)\n",
        "          & (F.col('InvoiceDate').isNotNull())\n",
        "          & (F.col('Description').isNotNull())\n",
        "          & (F.col('Description') != \" \")\n",
        "          & (F.trim(F.regexp_replace(F.col('StockCode'), '[^a-zA-Z]+', '')) == \"\")\n",
        "          )\n",
        "      .withColumn('Description', F.upper(F.trim(F.regexp_replace('Description', '[^a-zA-Z0-9]+', ' '))))\n",
        "      .withColumn('transactionDate',\n",
        "                  F.date_format(\n",
        "                      F.to_date(\n",
        "                          F.lpad(F.col('InvoiceDate').substr(1,10),10,'0'), 'MM/dd/yyyy')\n",
        "                      , 'yyyy-MM-dd'\n",
        "                      )\n",
        "                  )\n",
        "      .select(\n",
        "          F.col('CustomerID').cast('bigint').alias('customerID'),\n",
        "          F.col('Country').cast('string').alias('additionalSegmentation'),\n",
        "          F.col('InvoiceNo').cast('string').alias('invoiceNo'),\n",
        "          F.col('Description').cast('string').alias('productDescription'),\n",
        "          F.col('transactionDate'),\n",
        "          F.col('Quantity').cast('bigint').alias('productQuantity'),\n",
        "          F.col('UnitPrice').cast('double').alias('productPrice'),\n",
        "          )\n",
        "      .withColumn(\"monetaryValue\",(F.col('productPrice')*F.col('productQuantity')).cast('decimal(19,2)'))\n",
        "      .groupBy('customerID','additionalSegmentation','invoiceNo','productDescription','transactionDate')\n",
        "      .agg(\n",
        "          F.sum('productQuantity').cast('bigint').alias('productQuantityValue'),\n",
        "          F.sum('productPrice').cast('double').alias('productPriceValue'),\n",
        "          F.sum('monetaryValue').cast('double').alias('monetaryValue'),\n",
        "          )\n",
        "      # Separate dates into Year, month and days\n",
        "      .withColumn(\"transactionDateYear\", F.date_format(F.col(\"transactionDate\"), \"Y\"))\n",
        "      .withColumn(\"transactionDateMonth\", F.date_format(F.col(\"transactionDate\"), \"MM\"))\n",
        "      .withColumn(\"transactionDateDay\", F.date_format(F.col(\"transactionDate\"), \"dd\"))\n",
        "      .filter(\n",
        "          (F.col('productDescription').isNotNull())\n",
        "          & (~F.col('productDescription').isin(\"\",\" \"))\n",
        "          )\n",
        "      ).repartition(20).sortWithinPartitions(['customerID','transactionDate'])"
      ],
      "metadata": {
        "id": "wcSqF-dTMaDu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implicit value\n",
        "\n",
        "Note that we dont have a explicit value such as the \"like\" buttom on facebook, so I am going to create one based on the assumptions I made about the dataset. This is very important because it's going to change dramatically the outcome, but it also gives the business expert the flexibility to input the value that is mos important."
      ],
      "metadata": {
        "id": "qOMP86EEOGRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "\n",
        "# So here I am using the monetary value as the ranking.\n",
        "\n",
        "dfDescription = (df\n",
        "                 .groupBy('productDescription').agg(F.avg('monetaryValue').alias('monetaryValue'))\n",
        "                 )\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"monetaryValue\"],\n",
        "    outputCol=\"monetaryValueAssembler\")\n",
        "\n",
        "dfDescription = assembler.transform(dfDescription)\n",
        "\n",
        "scaler = MinMaxScaler(inputCol=\"monetaryValueAssembler\", outputCol=\"rankedScaled\")\n",
        "\n",
        "# Compute summary statistics and generate MinMaxScalerModel\n",
        "scalerModel = scaler.fit(dfDescription)\n",
        "\n",
        "# rescale each feature to range [min, max].\n",
        "dfDescription = scalerModel.transform(dfDescription)\n",
        "\n",
        "# Be aware that if this list of products changes, youre going to need to run all that again\n",
        "dfDescription = (dfDescription.withColumn('productID', F.row_number()\n",
        "         .over(Window\n",
        "               .orderBy(\"productDescription\")\n",
        "              )))"
      ],
      "metadata": {
        "id": "kk9LLD2hOFHu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NemxeyUtLOSb",
        "outputId": "6ea0dd04-4932-4f66-e280-fd2fb564992b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root-mean-square error = 2.0634193690818936\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import lit, udf\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "def ith_(v, i):\n",
        "    try:\n",
        "        return float(v[i])\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "ith = udf(ith_, DoubleType())\n",
        "\n",
        "dfCollab = (df\n",
        "            .join(\n",
        "                dfDescription.select('productDescription','productID',ith(\"rankedScaled\", lit(0)).cast('decimal(19,5)').alias('rankID'))\n",
        "                , on = ['productDescription']\n",
        "                , how = 'inner'\n",
        "                )\n",
        "            .withColumn('userID', F.col('customerID').cast('int'))\n",
        "            )\n",
        "\n",
        "(training, test) = dfCollab.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Build the recommendation model using ALS on the training data\n",
        "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
        "als = ALS(maxIter=5, regParam=0.01, userCol=\"userID\", itemCol=\"productID\", ratingCol=\"rankID\", implicitPrefs=True,\n",
        "          coldStartStrategy=\"drop\")\n",
        "model = als.fit(training)\n",
        "\n",
        "# Evaluate the model by computing the RMSE on the test data\n",
        "predictions = model.transform(test)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rankID\",\n",
        "                                predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root-mean-square error = \" + str(rmse))\n",
        "\n",
        "# Generate top 3 product recommendations for each user\n",
        "userRecs = model.recommendForAllUsers(3)\n",
        "# Generate top 3 user recommendations for each product\n",
        "productRecs = model.recommendForAllItems(3)\n",
        "\n",
        "# Generate top 10 product recommendations for a specified set of users\n",
        "users = dfCollab.select(als.getUserCol()).distinct().limit(10)\n",
        "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
        "# Generate top 10 user recommendations for a specified set of product\n",
        "products = dfCollab.select(als.getItemCol()).distinct().limit(10)\n",
        "productsSubSetRecs = model.recommendForItemSubset(products, 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "userRecs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ0E3pKsTKma",
        "outputId": "9dc138bb-9844-4b6a-e4c5-5c11aa1822cb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[userID: int, recommendations: array<struct<productID:int,rating:float>>]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets get the first item that we recommend for each customer\n",
        "transformedDf = (userRecs\n",
        "                 .withColumn(\"recommendedStruct\", F.col('recommendations').getItem(0))\n",
        "                 .select('userID',\n",
        "                         'recommendedStruct',\n",
        "                         F.col(\"recommendedStruct.productID\").alias(\"productID\"),\n",
        "                         )\n",
        "                 .join(\n",
        "                    dfDescription\n",
        "                    , on = ['productID']\n",
        "                    , how = 'left'\n",
        "                 )\n",
        "                 .select(F.col('userID').alias('customerID'),F.col('productDescription').alias('recommendedProduct'))\n",
        "                 )"
      ],
      "metadata": {
        "id": "3UrWN_ShRPA4"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformedDf.show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1rMbSdDT_PK",
        "outputId": "8e7bea53-f1a4-4f0d-9be0-1770ffa9c8a2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------------------+\n",
            "|userID|recommendedProduct            |\n",
            "+------+------------------------------+\n",
            "|12346 |CHILLI LIGHTS                 |\n",
            "|12347 |REGENCY CAKESTAND 3 TIER      |\n",
            "|12348 |PAPER CHAIN KIT 50 S CHRISTMAS|\n",
            "|12349 |REGENCY CAKESTAND 3 TIER      |\n",
            "|12350 |HAND OVER THE CHOCOLATE SIGN  |\n",
            "|12352 |REGENCY CAKESTAND 3 TIER      |\n",
            "|12353 |GIN TONIC DIET METAL SIGN     |\n",
            "|12354 |LUNCH BAG RED RETROSPOT       |\n",
            "|12355 |REGENCY CAKESTAND 3 TIER      |\n",
            "|12356 |REGENCY CAKESTAND 3 TIER      |\n",
            "+------+------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}